{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMVuSwV+agi0yDyH9iwhKpU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qS1v4QRVWTHg"},"outputs":[],"source":["class Ensemble(object):\n","  #앙상블 모델.\n","    def __init__(self, n_splits, stacker, base_models):\n","        self.n_splits = n_splits\n","        self.stacker = stacker\n","        self.base_models = base_models\n","\n","#x값, target 값, 테스트 값,\n","    def fit_predict(self, X, y, T):\n","        X = np.array(X)\n","        y = np.array(y)\n","        T = np.array(T)\n","\n","      #kfold를 해서 x,y로 나누겠다.\n","        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=314).split(X, y))\n","      #행의 수 (샘플의 수) * 모델의 수\n","        S_train = np.zeros((X.shape[0], len(self.base_models)))\n","        S_test = np.zeros((T.shape[0], len(self.base_models)))\n","        for i, clf in enumerate(self.base_models):\n","\n","            S_test_i = np.zeros((T.shape[0], self.n_splits))\n","\n","            for j, (train_idx, test_idx) in enumerate(folds):\n","              #kfold한 것 중 train 가져오기\n","                X_train = X[train_idx]\n","                y_train = y[train_idx]\n","                X_holdout = X[test_idx]\n","\n","\n","                print (\"Base model %d: fit %s model | fold %d\" % (i+1, str(clf).split('(')[0], j+1))\n","                #clf를 학습시킨다.\n","                clf.fit(X_train, y_train)\n","                #지정된 모델에 대해 교차검증한다.\n","                cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')\n","                print(\"cross_score [roc-auc]: %.5f [gini]: %.5f\" % (cross_score.mean(), 2*cross_score.mean()-1))\n","                #test에 대하여 예측한다.\n","                y_pred = clf.predict_proba(X_holdout)[:,1]                \n","                #해당 행, 해당 모델에 예측값 넣기.\n","                S_train[test_idx, i] = y_pred\n","                #train 하던거처럼 test로, [:,]\n","                #위 코드는 S_test_i 배열에서 j번째 열(column)에 해당하는 모든 행(row)의 값을 추출합니다.\n","                S_test_i[:, j] = clf.predict_proba(T)[:,1]\n","            S_test[:, i] = S_test_i.mean(axis=1)\n","\n","        results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')\n","        # Calculate gini factor as 2 * AUC - 1\n","        print(\"Stacker score [gini]: %.5f\" % (2 * results.mean() - 1))\n","\n","        self.stacker.fit(S_train, y)\n","        res = self.stacker.predict_proba(S_test)[:,1]\n","        return res"]},{"cell_type":"code","source":["lgb_params1 = {}\n","lgb_params1['learning_rate'] = 0.02\n","lgb_params1['n_estimators'] = 650\n","lgb_params1['max_bin'] = 10\n","lgb_params1['subsample'] = 0.8\n","lgb_params1['subsample_freq'] = 10\n","lgb_params1['colsample_bytree'] = 0.8   \n","lgb_params1['min_child_samples'] = 500\n","lgb_params1['seed'] = 314\n","lgb_params1['num_threads'] = 4\n","\n","# lgb2\n","lgb_params2 = {}\n","lgb_params2['n_estimators'] = 1090\n","lgb_params2['learning_rate'] = 0.02\n","lgb_params2['colsample_bytree'] = 0.3   \n","lgb_params2['subsample'] = 0.7\n","lgb_params2['subsample_freq'] = 2\n","lgb_params2['num_leaves'] = 16\n","lgb_params2['seed'] = 314\n","lgb_params2['num_threads'] = 4\n","\n","# lgb3\n","lgb_params3 = {}\n","lgb_params3['n_estimators'] = 1100\n","lgb_params3['max_depth'] = 4\n","lgb_params3['learning_rate'] = 0.02\n","lgb_params3['seed'] = 314\n","lgb_params3['num_threads'] = 4\n","\n","# XGBoost params\n","xgb_params = {}\n","xgb_params['objective'] = 'binary:logistic'\n","xgb_params['learning_rate'] = 0.04\n","xgb_params['n_estimators'] = 490\n","xgb_params['max_depth'] = 4\n","xgb_params['subsample'] = 0.9\n","xgb_params['colsample_bytree'] = 0.9  \n","xgb_params['min_child_weight'] = 10\n","xgb_params['num_threads'] = 4"],"metadata":{"id":"oucF8KLsaQ_3"},"execution_count":null,"outputs":[]}]}